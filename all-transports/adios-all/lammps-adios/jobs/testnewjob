#!/bin/bash
#    Begin PBS directives
#PBS -A csc143
#PBS -N lammps-adios 
#PBS -q debug
#PBS -j oe
#PBS -l walltime=00:40:00,nodes=2
#PBS -l gres=atlas1%atlas2
#    End PBS directives and begin shell commands

echo " 1=${1} 2=${2} nnode=2 napp1=32 napp2=16 trans=SIMULATION_ONLY x=256"

NSTOP=10

# procs placement
num_apps=2

# slots used by this app
procs_this_app=(32 16)

# number of nodes used by this app
nodes_this_app=(4 2)


PBS_O_HOME=${QZLPROJ}
PBS_O_WORKDIR=${QZLPROJ}/workflow-bench-master
export SCRATCH_DIR=${QZLPROJ}/sc15/results/lammps_adios_test/${procs_this_app[0]}v${procs_this_app[1]}_256_lammps_qzltest
#export SCRATCH_DIR=${QZLPROJ}/sc15/results/lammps_adios/${procs_this_app[0]}v${procs_this_app[1]}_lammps_sensei

rm -rf $SCRATCH_DIR
sleep 1s;
mkdir -pv $SCRATCH_DIR

#################################################### 

export CMTransport=nnti

export MyTransport=SIMULATION_ONLY

export OMP_NUM_THREADS=1

export CM_INTERFACE=ib0

BUILD_DIR=${PBS_O_WORKDIR}/build_new
#DS_SERVER=/sw/xk6/dataspaces/1.6.2/cle5.2_pgi16.10.0/bin/dataspaces_server
## Run DataSpaces servers
#CMD_SERVER="${LAUNCHER}  -n ${procs_this_app[0]} -o 0 ${DS_BIN}/dataspaces_server -s ${procs_this_app[0]} -c $((${procs_this_app[1]}+${procs_this_app[2]}))"
#$CMD_SERVER >& $PBS_RESULTDIR/server.log &
#echo "server applciation lauched: $CMD_SERVER"
## Give some time for the servers to load and startup
#while [ ! -f conf ]; do
#    sleep 1s
#done
#sleep 5s  # wait server to fill up the conf file

: <<'END'
env|grep '^CM'
env|grep '^HAS' # trace enabled?
env|grep '^OMP' # trace enabled?


module list

if [ x"$HAS_TRACE" == "x" ];then
    BUILD_DIR=${PBS_O_WORKDIR}/build
    DS_SERVER=${WORK}/envs/gcc_mvapich/Dataspacesroot/bin/dataspaces_server

elif [ x"$HAS_TRACE" = "xitac" ]; then
    #export LD_PRELOAD=libVT.so
    NSTOP=10
    echo "itac ENABLED, use 10 steps"
    export BUILD_DIR=${PBS_O_WORKDIR}/build_itac
    echo "use itac"
    export VT_LOGFILE_PREFIX=${SCRATCH_DIR}/trace 
    export VT_VERBOSE=3
    mkdir -pv $VT_LOGFILE_PREFIX

else
    echo "TRACE ENABLED"
    BUILD_DIR=${PBS_O_WORKDIR}/build_tau
    DS_SERVER=${WORK}/envs/Dataspacesroot_tau/bin/dataspaces_server
    #enable trace
    export TAU_TRACE=1
    # set trace dir
    export ALL_TRACES=${SCRATCH_DIR}/trace
    mkdir -pv $ALL_TRACES/app0
    mkdir -pv $ALL_TRACES/app1
    mkdir -pv $ALL_TRACES/app2

    if [ -z $TAU_MAKEFILE ]; then
        module load tau
        echo "LOAD TAU!"
    fi

fi


END
echo "procs is \[ ${procs_this_app[*]}\], nodes is \[${nodes_this_app[*]}\]"

#This job runs with 3 nodes  
#ibrun in verbose mode will give binding detail  #BUILD=${PBS_O_WORKDIR}/build_dspaces/bin
PBS_RESULTDIR=${SCRATCH_DIR}



mkdir -pv ${PBS_RESULTDIR}
tune_stripe_count=-1
lfs setstripe --stripe-size 1m --stripe-count ${tune_stripe_count} ${PBS_RESULTDIR}
mkdir -pv ${SCRATCH_DIR}
cd ${SCRATCH_DIR}
#cp -R ${PBS_O_WORKDIR}/global_range_select/arrays.xml ${SCRATCH_DIR}

cp -R ${PBS_O_WORKDIR}/all-transports/adios-all/lammps-adios/xmls ${SCRATCH_DIR}

: <<'END'

# this scrWorkspaces/General_Data_Broker/lbm_adios/scripts
GENERATE_HOST_SCRIPT=${PBS_O_WORKDIR}/scripts/generate_hosts.sh
#GENERATE_HOST_SCRIPT=${HOME}/Downloads/LaucherTest/generate_hosts.sh
if [ -a $GENERATE_HOST_SCRIPT ]; then
    source $GENERATE_HOST_SCRIPT
else
    echo "generate_hosts.sh should downloaded from:"
    echo "https://github.iu.edu/lifen/LaucherTest/blob/master/generate_hosts.sh"
fi
END

export procs_prod=${procs_this_app[0]}
#export procs_link=${procs_this_app[1]}
export procs_con=${procs_this_app[1]}

procs_all=$((procs_prod + procs_con))
#scripts/lammps_input/
infile=in.lj.32v16_256.txt
echo ${infile}
cp ${PBS_O_WORKDIR}/scripts/titan/lammps_input/$infile ./$infile

cmd_prod="$BUILD_DIR/bin/lammps_adios_prod $NSTOP $infile"
#cmd_con="$BUILD_DIR/bin/lammps_adios_con $NSTOP"


dsid=0
if [[ "$trans" == *"STAGING"* ]];then
    DS_SERVER=${DATASPACES_DIR}/bin/dataspaces_server
    MPI_DS="aprun  -n 32 -o 0 $DS_SERVER -s 32 -c $((16+32))"
#	$MPI_DS &> server.log &
	dsid=$!
	sleep 10s
fi

echo "ds pid is: $dsid"

## order is prod/link/consumer
MPI_CMD1="aprun    -n ${procs_prod}   $cmd_prod"

#MPI_CMD2="aprun  -n ${procs_con} valgrind --tool=massif --pages-as-heap=yes --time-unit=ms --massif-out-file=con.out.%p  $cmd_con"

export BP_DIR=$SCRATCH_DIR

#$MPI_CMD2 &> con.log &
#cpid=$!

$MPI_CMD1 &> prod.log &
ppid=$!
# Wait for the entire workflow to finish
wait 
kill -1 $ppid
kill -1 $dsid



