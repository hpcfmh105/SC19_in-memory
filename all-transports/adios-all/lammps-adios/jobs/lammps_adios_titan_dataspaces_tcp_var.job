#!/bin/bash
#    Begin PBS directives
#PBS -A csc143
#PBS -N lammps-adios 
#PBS -j oe
#PBS -q debug
#PBS -l walltime=00:40:00,nodes=${nnode}
#PBS -l gres=atlas1%atlas2
#    End PBS directives and begin shell commands

echo " 1=${1} 2=${2} nnode=${nnode} napp1=${napp1} napp2=${napp2} napp3=${napp3} trans=${trans} "

NSTOP=10

export DSPACES_GNI_PDOMAIN=ADIOS
export DSPACES_GNI_COOKIE=0x5420000
export DSPACES_GNI_PTAG=250

# procs placement
num_apps=2

# slots used by this app
procs_this_app=(${napp1} ${napp2})

# number of nodes used by this app
nodes_this_app=(4 2)


PBS_O_HOME=${MYPROJ}
PBS_O_WORKDIR=${MYPROJ}/workflow-bench-master
export SCRATCH_DIR=${MYPROJ}/sc15/results/lammps_adios/${procs_this_app[0]}v${procs_this_app[1]}v${napp3}_adios_ds_tcp_v2



#################################################### 

export MyTransport=${trans}

export OMP_NUM_THREADS=1

export CM_INTERFACE=ib0

BUILD_DIR=${PBS_O_WORKDIR}/build
#DS_SERVER=/sw/xk6/dataspaces/1.6.2/cle5.2_pgi16.10.0/bin/dataspaces_server
## Run DataSpaces servers
#CMD_SERVER="${LAUNCHER}  -n ${procs_this_app[0]} -o 0 ${DS_BIN}/dataspaces_server -s ${procs_this_app[0]} -c $((${procs_this_app[1]}+${procs_this_app[2]}))"
#$CMD_SERVER >& $PBS_RESULTDIR/server.log &
#echo "server applciation lauched: $CMD_SERVER"
## Give some time for the servers to load and startup
#while [ ! -f conf ]; do
#    sleep 1s
#done
#sleep 5s  # wait server to fill up the conf file

: <<'END'
env|grep '^CM'
env|grep '^HAS' # trace enabled?
env|grep '^OMP' # trace enabled?


module list

if [ x"$HAS_TRACE" == "x" ];then
    BUILD_DIR=${PBS_O_WORKDIR}/build
    DS_SERVER=${WORK}/envs/gcc_mvapich/Dataspacesroot/bin/dataspaces_server

elif [ x"$HAS_TRACE" = "xitac" ]; then
    #export LD_PRELOAD=libVT.so
    NSTOP=10
    echo "itac ENABLED, use 10 steps"
    export BUILD_DIR=${PBS_O_WORKDIR}/build_itac
    echo "use itac"
    export VT_LOGFILE_PREFIX=${SCRATCH_DIR}/trace 
    export VT_VERBOSE=3
    mkdir -pv $VT_LOGFILE_PREFIX

else
    echo "TRACE ENABLED"
    BUILD_DIR=${PBS_O_WORKDIR}/build_tau
    DS_SERVER=${WORK}/envs/Dataspacesroot_tau/bin/dataspaces_server
    #enable trace
    export TAU_TRACE=1
    # set trace dir
    export ALL_TRACES=${SCRATCH_DIR}/trace
    mkdir -pv $ALL_TRACES/app0
    mkdir -pv $ALL_TRACES/app1
    mkdir -pv $ALL_TRACES/app2

    if [ -z $TAU_MAKEFILE ]; then
        module load tau
        echo "LOAD TAU!"
    fi

fi


END
echo "procs is \[ ${procs_this_app[*]}\], nodes is \[${nodes_this_app[*]}\]"

#This job runs with 3 nodes  
#ibrun in verbose mode will give binding detail  #BUILD=${PBS_O_WORKDIR}/build_dspaces/bin
PBS_RESULTDIR=${SCRATCH_DIR}



mkdir -pv ${PBS_RESULTDIR}
tune_stripe_count=-1
lfs setstripe --stripe-size 1m --stripe-count ${tune_stripe_count} ${PBS_RESULTDIR}
mkdir -pv ${SCRATCH_DIR}
cd ${SCRATCH_DIR}
#cp -R ${PBS_O_WORKDIR}/global_range_select/arrays.xml ${SCRATCH_DIR}

cp -R ${PBS_O_WORKDIR}/all-transports/adios-all/lammps-adios/xmls ${SCRATCH_DIR}
rm conf
rm dataspaces.conf
rm massif*
sleep 1s
: <<'END'

# this scrWorkspaces/General_Data_Broker/lbm_adios/scripts
GENERATE_HOST_SCRIPT=${PBS_O_WORKDIR}/scripts/generate_hosts.sh
#GENERATE_HOST_SCRIPT=${HOME}/Downloads/LaucherTest/generate_hosts.sh
if [ -a $GENERATE_HOST_SCRIPT ]; then
    source $GENERATE_HOST_SCRIPT
else
    echo "generate_hosts.sh should downloaded from:"
    echo "https://github.iu.edu/lifen/LaucherTest/blob/master/generate_hosts.sh"
fi
END

export procs_prod=${procs_this_app[0]}
#export procs_link=${procs_this_app[1]}
export procs_con=${procs_this_app[1]}

procs_all=$((procs_prod + procs_con))
#scripts/lammps_input/
infile=in.lj.${napp1}v${napp2}.txt
cp ${PBS_O_WORKDIR}/scripts/titan/lammps_input/$infile ./$infile

cmd_prod="$BUILD_DIR/bin/lammps_adios_prod $NSTOP $infile 512000"
cmd_con="$BUILD_DIR/bin/lammps_adios_con $NSTOP "

dsid=0
if [[ ${trans} == *STAGING* ]];then

	echo "## Config file for DataSpaces
	ndim = 3 
	dims = $((5)),${napp1},$((512000))
	max_versions = 1
	hash_version = 2
	" > dataspaces.conf
	echo "DS_LIMIT= $DS_LIMIT"
    DS_SERVER=${DATASPACES_DIR}/bin/dataspaces_server
    MPI_DS=" aprun  -n ${napp3} -S 1 -m 12g $DS_SERVER -s ${napp3} -c $((${napp2} + ${napp1}))"
   # MPI_DS=" aprun  -n ${napp3} -S 1 -m 8g valgrind --tool=massif --pages-as-heap=yes --time-unit=B  $DS_SERVER -s ${napp3} -c $((${napp2} + ${napp1}))"
    $MPI_DS &> server.log &
    dsid=$!
	while [ ! -f conf ]; do
    	sleep 2s
	done
    sleep 10s

	while read line; do
		export set "${line}"
		echo "export ${line}"
	done < conf
fi
echo "$MPI_DS ds pid is: $dsid"
echo "-- DataSpaces IDs: P2TNID = $P2TNID   P2TPID = $P2TPID"


## order is prod/link/consumer
MPI_CMD1="aprun    -n ${procs_prod} $cmd_prod"

MPI_CMD2="aprun  -n ${procs_con} $cmd_con"

export BP_DIR=$SCRATCH_DIR

$MPI_CMD2 &> con.log &
cpid=$!

$MPI_CMD1 &> prod.log &
ppid=$!




# Wait for the entire workflow to finish
#wait $ppid
wait $cpid
kill -1 $ppid
kill -1 $dsid
echo "finish this job $PBS_JOBNAME"


